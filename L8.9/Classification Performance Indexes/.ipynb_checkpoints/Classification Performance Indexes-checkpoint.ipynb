{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chinese-gathering",
   "metadata": {},
   "source": [
    "## Commonly Used Classification Performance Indexes\n",
    "\n",
    "COSI/ECON-148B-1 : Introduction to Machine Learning with Economic Applications\n",
    "\n",
    "Prepared by Zizhang Chen ([zizhang2@brandeis.edu](mailto:zizhang2@brandeis.edu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-spencer",
   "metadata": {},
   "source": [
    "### 1. Overview\n",
    "The performance indexes of a machine learning model are computed on a given dataset. They can be used to \n",
    "\n",
    "- Compare and select models.\n",
    "\n",
    "- Tuning model parameters. \n",
    "\n",
    "- Track the training progress of a model. \n",
    "\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-editor",
   "metadata": {},
   "source": [
    "### 2. Performance Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-hearing",
   "metadata": {},
   "source": [
    "#### 2.1 Accuracy\n",
    "Measures the proportion of samples that were correctly classified by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"Training.csv\")\n",
    "data_test = pd.read_csv(\"Test.csv\")\n",
    "\n",
    "# data_train = pd.read_csv(\"Training_unbalanced.csv\")\n",
    "# data_test = pd.read_csv(\"Test_unbalanced.csv\")\n",
    "\n",
    "X_train = data_train[['Feature1', 'Feature2']]\n",
    "Y_train = data_train['Label']\n",
    "X_test = data_test[['Feature1', 'Feature2']]\n",
    "Y_test = data_test['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tribal-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(C = 10, solver = 'newton-cg').fit(X_train, Y_train)\n",
    "Y_pred = clf.predict( X_test )\n",
    "\n",
    "print( 'Accuracy = ', sum( Y_pred == Y_test ) / len( Y_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-couple",
   "metadata": {},
   "source": [
    "#### 2.2 Confusion table (Binary Classification)\n",
    "- True Positive (TP) refers to the number of instances that are correctly classified as positive. In a binary classification problem, positive usually represents the class of interest. For example, in a disease diagnosis problem, a positive result means the patient has the disease.\n",
    "\n",
    "- False Positive (FP) refers to the number of instances that are incorrectly classified as positive. In other words, it is a type of error in which a negative instance is predicted as positive.\n",
    "\n",
    "- True Negative (TN) refers to the number of instances that are correctly classified as negative. In a binary classification problem, negative usually represents the class not of interest. For example, in a disease diagnosis problem, a negative result means the patient does not have the disease.\n",
    "\n",
    "- False Negative (FN) refers to the number of instances that are incorrectly classified as negative. It is a type of error in which a positive instance is predicted as negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "columns = [\"Predicted 0\", \"Predicted 1\"]\n",
    "rows = [\"Actual 0\", \"Actual 1\"]\n",
    "\n",
    "plt.imshow( cm )\n",
    "\n",
    "# Show all ticks and label them with the respective list entries\n",
    "plt.xticks(np.arange(len(columns)), labels=columns)\n",
    "plt.yticks(np.arange(len(rows)), labels=rows)\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(columns)):\n",
    "    for j in range(len(rows)):\n",
    "        if i == j:\n",
    "            c = 'black'\n",
    "        else:\n",
    "            c = 'white'\n",
    "        text = plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\", color=c)\n",
    "\n",
    "plt.title( 'Confusion Matrix' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = cm.ravel()\n",
    "print('True positive:', tp)\n",
    "print('True negative:', tn)\n",
    "print('False positive:', fp)\n",
    "print('False negative:', fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TP, TN, FP, FN by yourself\n",
    "TP = sum((Y_pred == 1) & (Y_test == 1))\n",
    "TN = sum((Y_pred == 0) & (Y_test == 0))\n",
    "FP = sum((Y_pred == 1) & (Y_test == 0))\n",
    "FN = sum((Y_pred == 0) & (Y_test == 1))\n",
    "print('True positive:', TP)\n",
    "print('True negative:', TN)\n",
    "print('False positive:', FP)\n",
    "print('False negative:', FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-norman",
   "metadata": {},
   "source": [
    "#### 2.3 Precision, Recall, Specificity, and F1-score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moral-douglas",
   "metadata": {},
   "source": [
    "Precision, recall, sensitivity, and specificity are metrics used to evaluate the performance of binary classification algorithms.\n",
    "\n",
    "   - **Precision** measures the proportion of positive predictions that are actually positive. It is calculated as the number of true positive predictions divided by the sum of true positive and false positive predictions. Precision answers the question **\"Of all the positive predictions made by the classifier, what fraction of them are actually correct?\"**\n",
    "    \n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-homework",
   "metadata": {},
   "source": [
    "- **Recall**, also known as **sensitivity**, measures the proportion of positive samples that were correctly classified as positive. It is calculated as the number of true positive predictions divided by the sum of true positive and false negative predictions. Recall answers the question **\"Of all the positive samples in the dataset, what fraction of them were correctly classified as positive by the classifier?\"**\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-citizen",
   "metadata": {},
   "source": [
    "- **Specificity** also known as **selectivity** measures the proportion of negative samples that were correctly classified as negative. It is calculated as the number of true negative predictions divided by the sum of true negative and false positive predictions. Specificity answers the question **\"Of all the negative samples in the dataset, what fraction of them were correctly classified as negative by the classifier?\"**\n",
    "\n",
    "$$\n",
    "\\text{Specificity} = \\frac{TN}{FP + TN}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-wagon",
   "metadata": {},
   "source": [
    "- **F1-score** is the harmonic mean of precision and recall. It is a commonly used metric to summarize the performance of a binary classifier, especially when precision and recall are both important. \n",
    "\n",
    "$$\n",
    "\\text{F1-score} = \\frac{2 * Precision * Recall}{Precision * Recall}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-music",
   "metadata": {},
   "source": [
    "**Note:** that precision, recall, sensitivity, specificity, and F1-score are all defined in the context of binary classification algorithms, where the goal is to predict one of two possible classes. For multiclass classification problems, these metrics can be calculated for each class and averaged across classes to obtain a single summary of the classifier's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-resolution",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "digital-point",
   "metadata": {},
   "source": [
    "\n",
    "Write your own codes to calculate **Precision**, **Recall**, **Specificity**, and **F1-score**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-freight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "round-coast",
   "metadata": {},
   "source": [
    "#### 2.4 ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-acrylic",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation of the performance of a binary classifier system as its discrimination threshold is varied. It is a plot of the true positive rate against the false positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-arctic",
   "metadata": {},
   "source": [
    "$$ f(x) > t $$\n",
    "\n",
    "where $f()$ is the trained classifier, $x$ is a sample, and $t$ is the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import scipy.stats as stats # If you don't have scipy, install it now.\n",
    "\n",
    "Y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  Y_pred_proba)\n",
    "plt.plot(fpr,tpr, marker='.', label='Logistic regression')\n",
    "plt.plot(np.linspace(0, 1, 10), np.linspace(0, 1, 10), label='Random guess')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-thailand",
   "metadata": {},
   "source": [
    "### 2.5 AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-intersection",
   "metadata": {},
   "source": [
    "The **AUC** is the area under the ROC curve and summarizes the overall performance of the classifier. A higher AUC value indicates a better classifier, with a value of 1.0 representing a perfect classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-gross",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_proba = clf.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(Y_test,  Y_pred_proba)\n",
    "plt.plot(fpr,tpr, marker='.', label='Logistic regression')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.fill_between(fpr,tpr, step=\"pre\", alpha=0.4)\n",
    "plt.text(0.4, 0.5, 'AUC', dict(size=20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-sample",
   "metadata": {},
   "source": [
    "#### 2.5 Get a summary report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, Y_pred, digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-mitchell",
   "metadata": {},
   "source": [
    "### 3. Choices of Performance Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-demonstration",
   "metadata": {},
   "source": [
    "#### 3.1 Example 1 - Fraud detection\n",
    "\n",
    "Two classes: **Fraud** vs **Normal**\n",
    "\n",
    "TP: our model successfully detects fraud action. (cost = $0 per case)\n",
    "\n",
    "TN: our model successfully detects normal action. (cost = $0 per case)\n",
    "\n",
    "FP: our model misclassifies normal actions as fraud. (cost = $10 per mistake)\n",
    "\n",
    "FN: our model misclassifies fraud actions as normal. (cost = $10000 per mistake)\n",
    "\n",
    "In this case, it is crucial to minimize **false negatives**, as failing to detect a fraudulent transaction could result in significant financial consequences, and **recall** is the preferred metric than the precision, as it measures the proportion of actual positive instances that were correctly identified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-lecture",
   "metadata": {},
   "source": [
    "#### 3.2 Example 2 -- Spam filtering\n",
    "\n",
    "Two classes: **Spam email** vs **Normal email**\n",
    "\n",
    "TP: our model successfully detects Spam email. (cost = $0 per case)\n",
    "\n",
    "TN: our model successfully detects Normal email. (cost = $0 per case)\n",
    "\n",
    "**FP**: our model classifies Normal emails as Spam emails. (cost = $100 per mistake)\n",
    "\n",
    "FN: our model classifies Spam emails as Normal emails. (cost = $0.1 per mistake)\n",
    "\n",
    "In this case, it is crucial to minimize **false positives**, as failing to detect a common email could miss some important information, and **precision** is the preferred metric than the recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-ethiopia",
   "metadata": {},
   "source": [
    "#### 3.3 Discussion: Medical Diagnosis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-pursuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
